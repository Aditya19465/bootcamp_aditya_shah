{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 14: Deployment & Monitoring\n",
    "\n",
    "Use this template to draft your reflection and (optionally) sketch a dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reflection (200–300 words)\n",
    "- Risks if deployed:\n",
    "- Monitoring metrics across layers (Data/Model/System/Business):\n",
    "- Ownership & handoffs:\n",
    "\n",
    "> Tip: Be specific (e.g., 'p95 latency > 250ms triggers on-call notification')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f2000",
   "metadata": {},
   "source": [
    "Deploying a machine learning model in production introduces several risks that need careful monitoring across multiple layers. At the data layer, schema changes or sudden spikes in missing values can cause downstream failures; monitoring metrics like % nulls by column or schema hash mismatch ensures rapid detection. At the model layer, concept drift or silent degradation can occur if the underlying data distribution changes; tracking rolling metrics such as rolling MAE allows proactive intervention. At the system layer, performance bottlenecks could manifest as latency spikes or failed requests; p95 latency (ms) and error rates are key indicators. Finally, at the business layer, deviations in key performance indicators (KPIs) such as approval rate or revenue per decision signal that the model may be misaligned with objectives.\n",
    "\n",
    "Ownership and handoffs are critical: data engineers own the data pipeline and schema checks, ML engineers own model monitoring, system engineers monitor latency and availability, and product or business owners track KPI alignment. Each alert should map to clear responsibilities. For example, if p95 latency > 250ms, the on-call system engineer is notified; if rolling MAE > 0.12, the ML engineer investigates model drift. Metrics and thresholds must be well-documented, reproducible, and incorporated into dashboards and automated alerts. Continuous logging, versioning, and clear runbooks reduce the risk of silent failures and ensure that production issues are quickly diagnosed and mitigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Optional: Dashboard Sketch\n",
    "Describe panels and key charts. You can also attach an image file in your repo (png/pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional helper: simple structure to list metrics\n",
    "monitoring = {\n",
    "    'data': ['freshness_minutes', 'null_rate', 'schema_hash'],\n",
    "    'model': ['rolling_mae_or_auc', 'calibration_error'],\n",
    "    'system': ['p95_latency_ms', 'error_rate'],\n",
    "    'business': ['approval_rate', 'bad_rate']\n",
    "}\n",
    "monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55df44c",
   "metadata": {},
   "source": [
    "The conceptual dashboard should include panels for Data, Model, System, and Business layers, as well as a runbook reference:\n",
    "\n",
    "Layer\tMetrics\tPanel / Chart Ideas\n",
    "Data\tfreshness_minutes, % nulls, schema hash\tLine chart for freshness over time, bar chart for % nulls by column\n",
    "Model\trolling MAE/AUC, calibration error, stability index\tRolling line chart of MAE or AUC, histogram of prediction errors, calibration plot\n",
    "System\tp95 latency (ms), error rate, availability\tTime-series chart of latency, stacked bar of errors, uptime gauge\n",
    "Business\tapproval rate, bad-rate, revenue per decision\tKPI tiles showing trends, delta vs. baseline, sparkline charts\n",
    "Runbook\talerts, remediation steps, suppression windows\tTable or expandable text blocks linked to thresholds\n",
    "\n",
    "Example conceptual behavior:\n",
    "\n",
    "rolling_mae > 0.12 → highlight model panel with alert color.\n",
    "\n",
    "p95_latency_ms > 250 → highlight system panel and trigger notifications.\n",
    "\n",
    "Hovering over metrics shows the recent windowed values (e.g., last 20 observations for rolling MAE).\n",
    "\n",
    "This structure ensures each stakeholder can quickly assess performance and act on alerts relevant to their ownership domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
